#wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt
#wget http://images.cocodataset.org/zips/train2017.zip
#wget https://github.com/WongKinYiu/yolov7/blob/main/cfg/training/yolov7.yaml

import cv2
import numpy as np

# Load the YOLOv7 configuration file
config_file = './yolov7/yolov7.yaml'

# Load the YOLOv7 weights file
weights_file = './yolov7/yolov7.pt'

# Load the YOLOv7 model in OpenCV
net = cv2.dnn.readNetFromDarknet(config_file, weights_file)

# Load the crowd analysis classes
classes = []
with open('./yolov7/coco.names', 'r') as f:
    classes = [line.strip() for line in f.readlines()]

# Set the minimum confidence level for detection
conf_threshold = 0.5

# Set the non-maximum suppression threshold
nms_threshold = 0.4

# Create a list to store the number of people detected in each frame
people_count = []

# Open the video stream
cap = cv2.VideoCapture('FYP/v.mp4')

while True:
    # Read a frame from the video stream
    ret, frame = cap.read()

    if not ret:
        break

    # Get the dimensions of the frame
    height, width, channels = frame.shape

    # Create a blob from the frame
    blob = cv2.dnn.blobFromImage(frame, 1/255, (416, 416), swapRB=True)

    # Set the blob as the input to the YOLOv7 network
    net.setInput(blob)

    # Run the forward pass through the network
    outs = net.forward(net.getUnconnectedOutLayersNames())

    # Initialize some lists to store the detected boxes, confidences, and class IDs
    boxes = []
    confidences = []
    class_ids = []

    # Loop over each output layer
    for out in outs:
        # Loop over each detection
        for detection in out:
            # Extract the class ID and confidence of the current detection
            scores = detection[5:]
            class_id = np.argmax(scores)
            confidence = scores[class_id]

            # Filter out weak detections below the confidence threshold
            if confidence > conf_threshold:
                # Scale the bounding box coordinates back to the original image size
                box = detection[:4] * np.array([width, height, width, height])
                (centerX, centerY, w, h) = box.astype("int")

                # Calculate the top-left corner of the bounding box
                x = int(centerX - (w / 2))
                y = int(centerY - (h / 2))

                # Update the lists of detected boxes, confidences, and class IDs
                boxes.append([x, y, int(w), int(h)])
                confidences.append(float(confidence))
                class_ids.append(class_id)

    # Perform non-maximum suppression to remove overlapping boxes
    indices = cv2.dnn.NMSBoxes(boxes,confidences, conf_threshold, nms_threshold)
    # Initialize a variable to count the number of people detected in the current frame
    people_detected = 0
    # Loop over the indices of the remaining boxes after non-maximum suppression
    for i in indices:
        i = i[0]
        # Check if the detected object is a person
        if classes[class_ids[i]] == 'person':
            # Draw the bounding box around the detected object
            box = boxes[i]
            x, y, w, h = box
            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)
            # Increment the people detected counter
            people_detected += 1

    # Add the number of people detected in the current frame to the people_count list
    people_count.append(people_detected)

    # Display the frame with the bounding boxes and the number of people detected
    cv2.imshow('Crowd Analysis', frame)
    cv2.putText(frame, f'People Count: {people_detected}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
    cv2.waitKey(1)
    cap.release()
    
cv2.destroyAllWindows()
total_people = sum(people_count)
print(f'Total People Count: {total_people}')